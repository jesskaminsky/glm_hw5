---
title: "glm_hw5"
author: "Jess Kaminsky"
date: "4/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(lubridate)
library(metRology)
library(ggplot2)
library(lme4)
library(dplyr)
```

## Chapter 7 - Problem 2

```{r}
#sex is 1 if female

nsims <- 1000
elevator <- c()

for (i in 1:nsims) {
  sex <- rbinom(10, 1 , 0.52)
  log_weight <- ifelse(sex == 0, rnorm(10, 5.13, 0.17), rnorm(10, 4.96, 0.20))
  weight = exp(log_weight)
  elevator[i] = sum(weight)
}

prob <- sum(elevator > 1750) / length(elevator)
```

P(elevator breaks) is approximately 0.05

## Chapter 7 - Problem 8
Inference for the ratio of parameters: a (hypothetical) study compares the costs and effectiveness of two different medical treatments.
•In the first part of the study, the difference in costs between treatments A and B is estimated at $600 per patient, with a standard error of $400, based on a regression with 50 degrees of freedom.
•In the second part of the study, the difference in effectiveness is estimated at 3.0 (on some relevant measure), with a standard error of 1.0, based on a regression with 100 degrees of freedom.
•For simplicity, assume that the data from the two parts of the study were collected independently

Inference is desired for the incremental cost-effectiveness ratio: the difference between the average costs of the two treatments, divied by the difference between their average effectiveness.

(a) Create 1000 simulation draws of the cost difference and effectiveness difference, and make a scatterplot of these draws

n = 52 - generate 52 obs with mean = 600, sd = 400
n = 102 - generate 102 with 3, 1
ex: a = 300 b = 900
predictor is trt
y is cost diff
#rmt function
```{r}
cost_diff <- rnorm(1000, mean = 600, sd = 400/sqrt(52))
eff_diff <- rnorm(1000, mean = 3, sd = 1/sqrt(102))

plot(cost_diff, eff_diff)
```
page 142
(b) Use simulation to come up with an estimate, 50% interval, and 95% intercal for the incremental cost-effectiveness ratio
```{r}

```
(c) Repeat this problem, changing the standard error on the difference in effectiveness to 2.0

```{r}

```

## Chapter 8 - Problem 1
Fitting the wrong model: suppose you have 100 data points that arose from the following model:y=3+0.1x1+0.5x2+ error, with errors having a t distribution with mean 0, scale 5, and 4 degrees of freedom. We shall explore the implications of fitting a standard linear regression to these data.

### Part A
```{r}
#generate fake data
x.1 <- 1:100
x.2 <- rbinom(n = 100, 1, 0.5)
y <- 3 + .1*x.1 + .5*x.2 + rnorm(100, 0, 1)

#run the linear model on the simulated data
model_81a <- lm(y~x.1 + x.2)

estimates <- summary(model_81a)$coefficients[,1]
stderr <- summary(model_81a)$coefficients[,2]
regression_CI <- cbind(Lower_68_CI = estimates - stderr, Upper_68_CI = estimates + stderr)


b.x0 <- 3
b.x1 <- 0.1
b.x2 <- 0.5

bse.x0 <- se.coef(model_81a)[1]
bse.x1 <- se.coef(model_81a)[2]
bse.x2 <- se.coef(model_81a)[3]

bhat.x0 <- coef(model_81a)[1]
bhat.x1<- coef(model_81a)[2]
bhat.x2 <- coef(model_81a)[3]

cover.x0 <- abs(b.x0 - bhat.x0) < bse.x0
cover.x1 <- abs(b.x1 - bhat.x1) < bse.x1
cover.x2 <- abs(b.x2 - bhat.x2) < bse.x2

coverage <- cbind(Intercept = cover.x0, x.1 =  cover.x1, x.2 = cover.x2)
dimnames(coverage)[[1]] = "CI Contains True Value"

coverage
regression_CI
```
 
We simulate data from the following model 

$y = 3 + 0.1x1 + 0.5x2 + \epsilon$ where $\epsilon \sim N(0,1)$

Then we fit a linear regression to these data and generate the following coefficient estimates:
```{r, echo = FALSE}
beta_est <- cbind(Intercept = bhat.x0, x.1 = bhat.x1, x.2 = bhat.x2)
dimnames(beta_est)[[1]] <- "Point Estimate"
beta_est
```

We can then use the standard error of each estimate to generate 68% confidence intervals for the point estimates of each coefficient.
```{r, echo = FALSE}
regression_CI
```
Therefore, we see the following results. The 68% confidence intervals do not cover the true value of the coefficent for each term in the model.
```{r, echo = FALSE}
coverage
```

### Part B

After running the simulation from the previous question 1000 times, I have calculated the confidence coverage for the 68% intervals for each of the three coefficients in the model. The probabilities that the 68% confidence interval will cover the true paramater for each of the coefficents are as follows. 
```{r, echo = FALSE}
cover <- matrix(nrow = 1000, ncol = 3)

for(s in 1:1000) {
  x.1b <- 1:100 
  x.2b <- rbinom(n = 100, 1, 0.5)
  yb <- 3 + .1*x.1b + .5*x.2b + rnorm(100, 0, 1)
  model_81b <- lm(yb ~ x.1b + x.2b)
  b <- c(b.x0, b.x1, b.x2)
  hat <- coef(model_81b)
  se <- se.coef(model_81b)
  cover[s,] <- abs(b-hat) < se
}

cover_probs <- apply(cover, 2, mean)
cover_probs <- as.matrix(cover_probs)
dimnames(cover_probs)[[1]] <- c("Intercept", "x.1", "x.2")
dimnames(cover_probs)[[2]] <- "68% Confidence Coverage"
cover_probs
```

### Part C

I have performed the same simulation as the problem above; however, I fit the linear regression with errors following a t-distribution with mean = 0, scale = 5, and df = 4. The 68% confidence coverage for the coeffiencts are presented below.

```{r, echo = FALSE}
coverT <- matrix(nrow = 1000, ncol = 3)

for(s in 1:1000) {
  x.1c <- 1:100 
  x.2c <- rbinom(n = 100, 1, 0.5)
  yc <- 3 + .1*x.1c + .5*x.2c + rt.scaled(100, df = 4, mean = 0, sd = 5)
  model_81c <- lm(yb ~ x.1c + x.2c)
  b <- c(b.x0, b.x1, b.x2)
  hat <- coef(model_81c)
  se <- se.coef(model_81c)
  coverT[s,] <- abs(b-hat) < se
}

cover_probsT <- apply(coverT, 2, mean)
cover_probsT <- as.matrix(cover_probsT)
dimnames(cover_probsT)[[1]] <- c("Intercept", "x.1", "x.2")
dimnames(cover_probsT)[[2]] <- "68% Confidence Coverage"
cover_probsT

```
By accurately fitting the linear model with t errors rather than normal errors, we increase our ability to accurately capture and predict the true value of the coefficients.

## Chapter 8 - Problem 4

### Part A

We will perform predictive simulation to generate 1000 datasets based on fitting a poisson regression model predicting number of unprotected sex acts at follow up from baseline hiv status using the risky behavior data. After generating the data sets, we are interested in how well the model fits our data. We can explore this by comparing the original dataset to the datasets generated by the model. We are interested in comparing the percent of observations equal to 0 and the percent that are greater than 10 in our simulated data.

```{r, echo = FALSE}
risky <- read.csv("risky_behaviors.csv")[-1]
model_4a <- glm(round(fupacts) ~ bs_hiv, family = poisson, data = risky)

X <- cbind(rep(1, length(risky$fupacts)), risky$bs_hiv)
sim4a <- sim(model_4a, 1000)
y4a <- array(NA, c(1000, length(risky$fupacts)))
for (s in 1:1000) {
  y4a.hat <- exp(X %*% sim4a@coef[s,])
  y4a[s,] <- rpois(length(risky$fupacts), y4a.hat)
}

zero_test <- function(x) {
  mean (x==0)
}

ten_test <- function(x) {
  mean(x > 10)
}

zero.rep <- rep(NA, 1000)
for(s in 1:1000) {
  zero.rep[s] <- zero_test(y4a[s,])
}

ten.rep <- rep(NA, 1000)
for(t in 1:1000){
  ten.rep[t] <- ten_test(y4a[t,])
}

cat(paste("Simulated Percent of observations equal to zero: ", round(mean(zero.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations equal to zero: ", round(mean(round(risky$fupacts) == 0)*100, 3), "% \n\n"))

cat(paste("Simulated Percent of observations greater than ten: ", round(mean(ten.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations greater than ten: ", round(mean(round(risky$fupacts)  > 10)*100, 3), "% \n"))
```

### Part B
We will now perform the same simulation as above, but we will fit the data to an overdispersed poission regression model. A comparison of the original and simulated datasets is presented below.

```{r, echo = FALSE}
model_4b <- glm(round(fupacts) ~ bs_hiv, family = quasipoisson, data = risky)

Xb <- cbind(rep(1, length(risky$fupacts)), risky$bs_hiv)
sim4b <- sim(model_4b, 1000)
y4b <- array(NA, c(1000, length(risky$fupacts)))
for (s in 1:1000) {
  y4b.hat <- exp(Xb %*% sim4b@coef[s,])
  y4b[s,] <- rpois(length(risky$fupacts), y4b.hat)
}


zero.rep <- rep(NA, 1000)
for(s in 1:1000) {
  zero.rep[s] <- zero_test(y4b[s,])
}


ten.rep <- rep(NA, 1000)
for(t in 1:1000){
  ten.rep[t] <- ten_test(y4b[t,])
}

cat(paste("Simulated Percent of observations equal to zero: ", round(mean(zero.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations equal to zero: ", round(mean(round(risky$fupacts) == 0)*100, 3), "% \n\n"))

cat(paste("Simulated Percent of observations greater than ten: ", round(mean(ten.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations greater than ten: ", round(mean(round(risky$fupacts)  > 10)*100, 3), "% \n"))
```
The simulated and actual percentages are closer than in the previous question. This indicated that the overdispersed poisson model is more appropriate for our dataset than the poisson model.

### Part C

Again, we will perfom the same simulation as above - using an overdispersed poisson model - but we will add in the covariate for baseline number of unprotected sex acts. A summary of the generated data is presented below.

```{r, echo = FALSE}
model_4c <- glm(round(fupacts) ~ bs_hiv + bupacts, family = quasipoisson, data = risky)

Xc <- cbind(rep(1, length(risky$fupacts)), risky$bs_hiv, risky$bupacts)
sim4c <- sim(model_4c, 1000)
y4c <- array(NA, c(1000, length(risky$fupacts)))
for (s in 1:1000) {
  y4c.hat <- exp(Xc %*% sim4c@coef[s,])
  y4c[s,] <- rpois(length(risky$fupacts), y4c.hat)
}


zero.rep <- rep(NA, 1000)
for(s in 1:1000) {
  zero.rep[s] <- zero_test(y4c[s,])
}


ten.rep <- rep(NA, 1000)
for(t in 1:1000){
  ten.rep[t] <- ten_test(y4c[t,])
}

cat(paste("Simulated Percent of observations equal to zero: ", round(mean(zero.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations equal to zero: ", round(mean(round(risky$fupacts) == 0)*100, 3), "% \n\n"))

cat(paste("Simulated Percent of observations greater than ten: ", round(mean(ten.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations greater than ten: ", round(mean(round(risky$fupacts)  > 10)*100, 3), "% \n"))

```
Our accuracy in generating observations equal to 0 has not improved. We are successful in generating the appropriate number of obervations greater than ten.


## Chapter 11 - Problem 4

### Part A

```{r}
cd4 <- read.csv("cd4.csv")

cd4$vdate <- mdy(cd4$vdate)
cd4_full <- cd4[complete.cases(cd4),]

ggplot(data=cd4_full, aes(x=vdate, y=sqrt(cd4pct), group=newpid, colour=factor(newpid)))+ 
    geom_line(size=.75) + geom_point() + theme(legend.position="none") + xlab("Year") + ylab("Square Root of CD4 Percentage") + ggtitle("Chapter 11 - Problem 4", subtitle = "Part A")
    
```
### Part B 
```{r}
attach(cd4_full)

plot(cd4$vdate[newpid == 1], sqrt(cd4$cd4pct[newpid ==1]), xlim = c(as.Date("1988-03-07"), as.Date("1991-01-14")), ylim = c(0, 10), xlab = "Year", ylab = "Square Root of CD4 Percentage", main = "Chapter 11 - Problem 4 \n Part B")


for(i in unique(cd4_full$newpid)) {
  #print(i)
  #abline(lm(sqrt(cd4pct) ~ vdate, data = cd4_full[newpid == i,]))
  temp_mod <- lm(sqrt(cd4pct) ~ vdate, data = cd4_full[newpid == i,])
  cd4_full$intercept[newpid == i] <- temp_mod$coefficients[1]
  cd4_full$slope[newpid == i] <- temp_mod$coefficients[2]
  coeffs <- coef(temp_mod)
   if(is.na(coeffs[2])) {
     coeffs[2] <- 0
   }
  abline(coeffs) 
  }
```
### Part C 
#coefficients of slope model are sooooo small
```{r}

cd4_unique <- cd4_full %>% group_by(newpid) %>% slice(1)
attach(cd4_unique)
slope_mod <- lm(slope ~ treatmnt + baseage, data = cd4_unique)
intercept_mod <- lm(intercept ~ treatmnt + baseage, data = cd4_unique)

display(slope_mod)
display(intercept_mod)

```

## Chapter 12 - Problem 2

### Part A

We generated the following multilevel model predicting cd4 percentage from time with an intercept varying among patients

$cd4\%=71.91 - 0.01*vdate$

The coefficient for time generated by =this multilevel model with varying intercepts across children is -0.01. This means that for each child CD4 percentage decreases by 0.01 percent for every unit increase in time. A brief summary of the model is presented below.

```{r}
attach(cd4_full)
model_122a <- lmer(cd4pct ~ vdate + (1|newpid), data = cd4_full)
display(model_122a)
```

### Part B 

After extending the previous model to include child-level predictors for treatment and age at baseline, the resulting model is presented below. The intercept term of this model varies by patient.

$cd4\%= 71.57 -0.01*vdate + 1.91*treatment - 0.96*baseage $

We can interpret the coefficients as follows:
- For every unit increase in visit date, we expect cd4 percentage to decrease by 0.01%
- Being in treatment group 2, increases your expected cd4 percentage by 1.91%
- For every 1 year increase in age at baseline, we expect cd4 percentage to decrease by 0.96%

A brief summary of the model from R is shown below.

```{r}
model_122b <- lmer(cd4pct ~ vdate + treatmnt + baseage + (1|newpid), data = cd4_full)

display(model_122b)
```

### Part C
(c) Investigate the change in partial pooling from (a) to (b) both graphically and numerically.
```{r}

coef_1 <- coef(model_122a)$newpid[[1]]
coef_2 <- coef(model_122b)$newpid[[1]]


# Change in standard errors


# Confidence intervals for each intercept for both moels
df1$ci_bottom <- df1$int + (-2 * se.ranef(mod1)$newpid[,1])
df1$ci_upper <- df1$int + (2 * se.ranef(mod1)$newpid[,1])

df2$ci_bottom <- df2$int + (-2 * se.ranef(mod2)$newpid[,1])
df2$ci_upper <- df2$int + (2 * se.ranef(mod2)$newpid[,1])

# Now we need to compare whether the CI's shrunk from
# the first to the second model

# Calculate difference
df1$diff <- df1$ci_upper - df1$ci_bottom
df2$dff <- df2$ci_upper - df2$ci_bottom

# Create a df with both differences
df3 <- data.frame(cbind(df$diff, df2$dff))

# Create a difference out of that
df3$diff <- df3$X1 - df3$X2

# Graph it
ggplot(df3, aes(diff)) + geom_histogram(bins = 100) +
  xlim(0, 0.2)

# It looks like the difference is always higher than zero which
# means that in the second model the difference between
# the upper and lower CI is smaller than in the first model.
# This suggests we have greater certainty of our estimation
# by including the two predictors in the model.

# Numerically, the between-child variance in the first
# model was:
display(mod1)
11.65 / (11.65 + 7.31)

# 0.614%

# For the second model
display(mod2)
11.45 / (11.45 + 7.32)

# 0.6100%

# The between variance went down JUST a little, in line
# with the tiny reduction in the standard errors of the intercept.
```
### Part D 

```{r}
#anova between 2 models
#SE on random effects
anova(model_122b, model_122a)
```

## Chapter 12 - Problem 3

### Part A

In order to create a hypothetical next timepoint for each patient I found the average time difference between their visits and added that difference to their last visit. If a patient only had one visit and therefore no difference between visit dates, I used the average of the average differences. Then, I used the model from above predicting cd4 prercentage, vist date, treatment group, and baseline age to predict cd4 percentage for each patient and their new hypothetical next timepoint. The predicted values for each patient are listed below.
###FIX THE DATES
```{r}
cd4_averages <- cd4_full %>% group_by(newpid) %>% arrange(vdate) %>% summarize(ave = as.numeric(mean(diff(vdate))))
  

cd4_newtime <- cd4_full %>% group_by(newpid) %>% filter(vdate == max(vdate))

for(i in 1:length(cd4_averages$ave)) {
  if(is.na(cd4_averages$ave[i])) {
    cd4_averages$ave[i] <- mean(cd4_averages$ave, na.rm = TRUE)
  }
} 

cd4_newtime$vdate <- cd4_newtime$vdate + cd4_averages$ave

newtime_predict <- predict(model_122b, cd4_newtime)
print(cbind(newpid = cd4_newtime$newpid, newdate = cd4_newtime$vdate, prediction = newtime_predict))
```
### Part B

While continuing to use the same predictive model, I will use the same dataset as the previous question but change each patients baseline age to 4 and generate new predictions for cd4 percentage.

```{r}
cd4_newtime4 <- cd4_newtime
cd4_newtime4$baseage <- 4

newtime_predict4 <- predict(model_122b, cd4_newtime4)
print(cbind(newpid = cd4_newtime4$newpid, newdate = cd4_newtime4$vdate, prediction = newtime_predict4))
```

## Chapter 12 - Problem 4

We will now generate more predictions - this time using the original, na omitted data except every patients visit date is replaced by the latest date represented in the dataset which is January 14, 1991. We can see the distribution of predicted CD4 percentage after 1000 simulations in the histogram below. The observed CD4 percentages at the final date in the dataset fall in the middle of the distribution which gives us confidence that we have a good model.

```{r}
final_time <- cd4_full
final_time$vdate <- max(final_time$vdate)

final_time <- final_time %>% group_by(newpid) %>% slice(1)

final_predict <- predict(model_122b, final_time)
cat(paste("Mean CD4 percentage from Simulated Data using Final date", round(mean(final_predict),3), "\n"))
cat(paste("Standard Deviation CD4 percentage from Simulated Data using Final date", round(sd(final_predict),3), "\n"))
hist(rnorm(1000, mean(final_predict), sd(final_predict)))

cat(paste("Observed CD4 Percentages on Max Date: ", cd4_full$cd4pct[vdate == max(vdate)][1], " and ", cd4_full$cd4pct[vdate == max(vdate)][2], "\n"))

cat(paste("Average CD4 Percentage on Max Date: ", mean(cd4_full$cd4pct[vdate == max(vdate)]), "\n"))
```

