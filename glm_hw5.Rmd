---
title: "glm_hw5"
author: "Jess Kaminsky"
date: "4/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(lubridate)
library(metRology)
library(ggplot2)
library(lme4)
```

## Chapter 7 - Problem 2

```{r}
#sex is 1 if female

nsims <- 1000
elevator <- c()

for (i in 1:nsims) {
  sex <- rbinom(10, 1 , 0.52)
  log_weight <- ifelse(sex == 0, rnorm(10, 5.13, 0.17), rnorm(10, 4.96, 0.20))
  weight = exp(log_weight)
  elevator[i] = sum(weight)
}

prob <- sum(elevator > 1750) / length(elevator)
```

P(elevator breaks) is approximately 0.05

## Chapter 7 - Problem 8
Inference for the ratio of parameters: a (hypothetical) study compares the costs and effectiveness of two different medical treatments.
•In the first part of the study, the difference in costs between treatments A and B is estimated at $600 per patient, with a standard error of $400, based on a regression with 50 degrees of freedom.
•In the second part of the study, the difference in effectiveness is estimated at 3.0 (on some relevant measure), with a standard error of 1.0, based on a regression with 100 degrees of freedom.
•For simplicity, assume that the data from the two parts of the study were collected independently

Inference is desired for the incremental cost-effectiveness ratio: the difference between the average costs of the two treatments, divied by the difference between their average effectiveness.

(a) Create 1000 simulation draws of the cost difference and effectiveness difference, and make a scatterplot of these draws
#WHAT ABOUT THE DF
n = 52
n = 102

divide by n-k + 1
```{r}
cost_diff <- rnorm(1000, mean = 600, sd = 400)
eff_diff <- rnorm(1000, mean = 3, sd = 1)

plot(cost_diff, eff_diff)
```

(b)
(c)

```{r}

```

## Chapter 8 - Problem 1
Fitting the wrong model: suppose you have 100 data points that arose from the following model:y=3+0.1x1+0.5x2+ error, with errors having a t distribution with mean 0, scale 5, and 4 degrees of freedom. We shall explore the implications of fitting a standard linear regression to these data.

### Part A
```{r}
#generate fake data
x.1 <- 1:100
x.2 <- rbinom(n = 100, 1, 0.5)
y <- 3 + .1*x.1 + .5*x.2 + rnorm(100, 0, 1)

#run the linear model on the simulated data
model_81a <- lm(y~x.1 + x.2)

estimates <- summary(model_81a)$coefficients[,1]
stderr <- summary(model_81a)$coefficients[,2]
regression_CI <- cbind(Lower_68_CI = estimates - stderr, Upper_68_CI = estimates + stderr)


b.x0 <- 3
b.x1 <- 0.1
b.x2 <- 0.5

bse.x0 <- se.coef(model_81a)[1]
bse.x1 <- se.coef(model_81a)[2]
bse.x2 <- se.coef(model_81a)[3]

bhat.x0 <- coef(model_81a)[1]
bhat.x1<- coef(model_81a)[2]
bhat.x2 <- coef(model_81a)[3]

cover.x0 <- abs(b.x0 - bhat.x0) < bse.x0
cover.x1 <- abs(b.x1 - bhat.x1) < bse.x1
cover.x2 <- abs(b.x2 - bhat.x2) < bse.x2

coverage <- cbind(Intercept = cover.x0, x.1 =  cover.x1, x.2 = cover.x2)
dimnames(coverage)[[1]] = "CI Contains True Value"

coverage
regression_CI
```
 
We simulate data from the following model 

$y = 3 + 0.1x1 + 0.5x2 + \epsilon$ where $\epsilon \sim N(0,1)$

Then we fit a linear regression to these data and generate the following coefficient estimates:
```{r, echo = FALSE}
beta_est <- cbind(Intercept = bhat.x0, x.1 = bhat.x1, x.2 = bhat.x2)
dimnames(beta_est)[[1]] <- "Point Estimate"
beta_est
```

We can then use the standard error of each estimate to generate 68% confidence intervals for the point estimates of each coefficient.
```{r, echo = FALSE}
regression_CI
```
Therefore, we see the following results. The 68% confidence intervals do not cover the true value of the coefficent for each term in the model.
```{r, echo = FALSE}
coverage
```

### Part B

After running the simulation from the previous question 1000 times, I have calculated the confidence coverage for the 68% intervals for each of the three coefficients in the model. The probabilities that the 68% confidence interval will cover the true paramater for each of the coefficents are as follows. 
```{r, echo = FALSE}
cover <- matrix(nrow = 1000, ncol = 3)

for(s in 1:1000) {
  x.1b <- 1:100 
  x.2b <- rbinom(n = 100, 1, 0.5)
  yb <- 3 + .1*x.1b + .5*x.2b + rnorm(100, 0, 1)
  model_81b <- lm(yb ~ x.1b + x.2b)
  b <- c(b.x0, b.x1, b.x2)
  hat <- coef(model_81b)
  se <- se.coef(model_81b)
  cover[s,] <- abs(b-hat) < se
}

cover_probs <- apply(cover, 2, mean)
cover_probs <- as.matrix(cover_probs)
dimnames(cover_probs)[[1]] <- c("Intercept", "x.1", "x.2")
dimnames(cover_probs)[[2]] <- "68% Confidence Coverage"
cover_probs
```

### Part C

I have performed the same simulation as the problem above; however, I fit the linear regression with errors following a t-distribution with mean = 0, scale = 5, and df = 4. The 68% confidence coverage for the coeffiencts are presented below.

```{r, echo = FALSE}
coverT <- matrix(nrow = 1000, ncol = 3)

for(s in 1:1000) {
  x.1c <- 1:100 
  x.2c <- rbinom(n = 100, 1, 0.5)
  yc <- 3 + .1*x.1c + .5*x.2c + rt.scaled(100, df = 4, mean = 0, sd = 5)
  model_81c <- lm(yb ~ x.1c + x.2c)
  b <- c(b.x0, b.x1, b.x2)
  hat <- coef(model_81c)
  se <- se.coef(model_81c)
  coverT[s,] <- abs(b-hat) < se
}

cover_probsT <- apply(coverT, 2, mean)
cover_probsT <- as.matrix(cover_probsT)
dimnames(cover_probsT)[[1]] <- c("Intercept", "x.1", "x.2")
dimnames(cover_probsT)[[2]] <- "68% Confidence Coverage"
cover_probsT

```
By accurately fitting the linear model with t errors rather than normal errors, we increase our ability to accurately capture and predict the true value of the coefficients.

## Chapter 8 - Problem 4

### Part A
<!-- (a)  Fit a Poisson regression model predicting number of unprotected sex acts from baseline HIV status. Perform predictive simulation to generate 1000 datasets and record both the percent of observations that are equal to 0 and the percent that are greater than 10 (the third quartile in the observed data) for each. Compare these values to the observed value in the original data. -->
<!-- #i dont see 10 as the 3rd quantile -->

We will perform predictive simulation to generate 1000 datasets based on fitting a poisson regression model predicting number of unprotected sex acts at follow up from baseline hiv status using the risky behavior data. After generating the data sets, we are interested in how well the model fits our data. We can explore this by comparing the original dataset to the datasets generated by the model. We are interested in comparing the percent of observations equal to 0 and the percent that are greater than 10 in our simulated data.

```{r, echo = FALSE}
risky <- read.csv("risky_behaviors.csv")[-1]
model_4a <- glm(round(fupacts) ~ bs_hiv, family = poisson, data = risky)

X <- cbind(rep(1, length(risky$fupacts)), risky$bs_hiv)
sim4a <- sim(model_4a, 1000)
y4a <- array(NA, c(1000, length(risky$fupacts)))
for (s in 1:1000) {
  y4a.hat <- exp(X %*% sim4a@coef[s,])
  y4a[s,] <- rpois(length(risky$fupacts), y4a.hat)
}

zero_test <- function(x) {
  mean (x==0)
}

ten_test <- function(x) {
  mean(x > 10)
}

zero.rep <- rep(NA, 1000)
for(s in 1:1000) {
  zero.rep[s] <- zero_test(y4a[s,])
}

ten.rep <- rep(NA, 1000)
for(t in 1:1000){
  ten.rep[t] <- ten_test(y4a[t,])
}

cat(paste("Simulated Percent of observations equal to zero: ", round(mean(zero.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations equal to zero: ", round(mean(round(risky$fupacts) == 0)*100, 3), "% \n\n"))

cat(paste("Simulated Percent of observations greater than ten: ", round(mean(ten.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations greater than ten: ", round(mean(round(risky$fupacts)  > 10)*100, 3), "% \n"))
```

### Part B
We will now perform the same simulation as above, but we will fit the data to an overdispersed poission regression model. A comparison of the original and simulated datasets is presented below.

```{r, echo = FALSE}
model_4b <- glm(round(fupacts) ~ bs_hiv, family = quasipoisson, data = risky)

Xb <- cbind(rep(1, length(risky$fupacts)), risky$bs_hiv)
sim4b <- sim(model_4b, 1000)
y4b <- array(NA, c(1000, length(risky$fupacts)))
for (s in 1:1000) {
  y4b.hat <- exp(Xb %*% sim4b@coef[s,])
  y4b[s,] <- rpois(length(risky$fupacts), y4b.hat)
}


zero.rep <- rep(NA, 1000)
for(s in 1:1000) {
  zero.rep[s] <- zero_test(y4b[s,])
}


ten.rep <- rep(NA, 1000)
for(t in 1:1000){
  ten.rep[t] <- ten_test(y4b[t,])
}

cat(paste("Simulated Percent of observations equal to zero: ", round(mean(zero.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations equal to zero: ", round(mean(round(risky$fupacts) == 0)*100, 3), "% \n\n"))

cat(paste("Simulated Percent of observations greater than ten: ", round(mean(ten.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations greater than ten: ", round(mean(round(risky$fupacts)  > 10)*100, 3), "% \n"))
```
The simulated and actual percentages are closer than in the previous question. This indicated that the overdispersed poisson model is more appropriate for our dataset than the poisson model.

### Part C
(c) also including ethnicity and baseline number of unprotected sex acts as input variables.
#we are not given a variable for ethnicity

Again, we will perfom the same simulation as above - using an overdispersed poisson model - but we will add in the covariate for baseline number of unprotected sex acts. A summary of the generated data is presented below.

```{r, echo = FALSE}
model_4c <- glm(round(fupacts) ~ bs_hiv + bupacts, family = quasipoisson, data = risky)

Xc <- cbind(rep(1, length(risky$fupacts)), risky$bs_hiv, risky$bupacts)
sim4c <- sim(model_4c, 1000)
y4c <- array(NA, c(1000, length(risky$fupacts)))
for (s in 1:1000) {
  y4c.hat <- exp(Xc %*% sim4c@coef[s,])
  y4c[s,] <- rpois(length(risky$fupacts), y4c.hat)
}


zero.rep <- rep(NA, 1000)
for(s in 1:1000) {
  zero.rep[s] <- zero_test(y4c[s,])
}


ten.rep <- rep(NA, 1000)
for(t in 1:1000){
  ten.rep[t] <- ten_test(y4c[t,])
}

cat(paste("Simulated Percent of observations equal to zero: ", round(mean(zero.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations equal to zero: ", round(mean(round(risky$fupacts) == 0)*100, 3), "% \n\n"))

cat(paste("Simulated Percent of observations greater than ten: ", round(mean(ten.rep)*100, 3), "% \n"))

cat(paste("Actual Percent of observations greater than ten: ", round(mean(round(risky$fupacts)  > 10)*100, 3), "% \n"))

```
Our accuracy in generating observations equal to 0 has not improved. We are successful in generating the appropriate number of obervations greater than ten.

## Chapter 11

### Problem 4
The foldercd4has CD4 percentages for a set of young children with HIV whowere measured several times over a period of two years. The dataset also includesthe ages of the children at each measurement.
(a) Graph the outcome (the CD4 percentage, on the square root scale) for eachchild as a function of time.
```{r}
cd4 <- read.csv("cd4.csv")

cd4$vdate <- mdy(cd4$vdate)
cd4_full <- cd4[complete.cases(cd4),]
#
plot(cd4$vdate[cd4$newpid == 1], sqrt(cd4$cd4pct[cd4$newpid == 1]))


ggplot(data=cd4_full, aes(x=vdate, y=sqrt(cd4pct), group=newpid, colour=factor(newpid)))+ 
    geom_line(size=.75) + geom_point() + theme(legend.position="none")

  
    # scale_x_discrete(limits=c("Jan","Feb","Mar","Apr","May","Jun","Jul",
    #     "Aug","Sep","Oct","Nov","Dec")) + 
    # scale_y_continuous(labels=comma) + 
    # scale_colour_manual(values=cPalette, name="Year") +    
    # ylab("Volume")
    
```
(b)  Each child’s data has a time course that can be summarized by a linear fit.Estimate these lines and plot them for all the children.
```{r}
ggplot(data=cd4, aes(sqrt(cd4pct)~vdate, group=newpid, colour=factor(newpid)))+ 
    geom_line(size=.75) + geom_point() + theme(legend.position="none")

scatterplot <- qplot(x=vdate, y=sqrt(cd4pct), data=cd4)
scatterplot + geom_abline(aes(intercept=intercept, slope=slope,
  colour=quantile), data=quantile.regressions)


plot(cd4$vdate[newpid == 1], sqrt(cd4$cd4pct[newpid ==1]), xlim = c(as.Date("1988-03-07"), as.Date("1991-01-14")), ylim = c(0, 10))

# 
# for(i in 1:max(cd4_full$newpid)) {
#   abline(lm(sqrt(cd4pct) ~ vdate, data = cd4_full[newpid == i,]))
# }


abline(lm(sqrt(cd4pct) ~ vdate, data = cd4[newpid == 1,]))
abline(lm(sqrt(cd4pct) ~ vdate, data = cd4[newpid == 2,]))
abline(lm(sqrt(cd4pct) ~ vdate, data = cd4[newpid == 3,]))
abline(lm(sqrt(cd4pct) ~ vdate, data = cd4[newpid == 4,]))
#intercept only model
abline(lm(sqrt(cd4pct) ~ vdate, data = cd4[newpid == 5,]))
abline(lm(sqrt(cd4pct) ~ vdate, data = cd4[newpid == 6,]))
```
(c)  Set  up  a  model  for  the  children’s  slopes  and  intercepts  as  a  function  ofthe treatment and age at baseline. Estimate this model using the two-step procedure–first estimate the intercept and slope separately for each child, thenfit the between-child models using the point estimates from the first step.
```{r}

```
## Chapter 12

### Problem 2
Continuing with the analysis of the CD4 data from Exercise 11.4:
(a) Write a model predicting CD4 percentage as a function of time with varyingintercepts across children. Fit usinglmer()and interpret the coefficient fortime.
```{r}

```
(b)  Extend the model in (a) to include child-level predictors (that is, group-levelpredictors) for treatment and age at baseline. Fit using lmer()and interpretthe coefficients on time, treatment, and age at baseline.
(c) Investigate the change in partial pooling from (a) to (b) both graphically andnumerically.
(d)  Compare results in (b) to those obtained in part (c).
```{r}

```
### Problem 3
Predictions for new observations and new groups:
(a) Use the model fit from Exercise 12.2(b) to generate simulation of predictedCD4 percentages for each child in the dataset at a hypothetical next timepoint.
(b)  Use the same model fit to generate simulations of CD4 percentages at each ofthe time periods for a new child who was 4 years old at baseline.
### Problem 4
Posterior predictive checking: continuing the previous exercise, use the fittedmodel from Exercise 12.2(b) to simulate a new dataset of CD4 percentages (withthe same sample size and ages of the original dataset) for the final time point ofthe study, and record the average CD4 percentage in this sample. Repeat thisprocess 1000 times and compare the simulated distribution to the observed CD4percentage at the final time point for the actual data.
```{r}

```